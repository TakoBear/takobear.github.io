<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>iPhone也能長曝光! AVFoundation與濾鏡的魔術</title>
  <meta name="description" content=" 這照片作為封面實在手震得很厲害....不過Bear希望能傳達一個重點! 就是如何利用iOS 的framework來製作出長曝光的效果!利用Bear 很早之前研究的一個framework - AVFoundation 以及CoreImage兩項核心就能完成了喔!下一篇Bear還會介紹如何使用音量鍵作為快門鍵的教學...">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/blog.css">
  <link rel="stylesheet" href="/css/paginate.css">
  <link rel="canonical" href="http://www.takobear.tw/2014/01/12/iphone-avfoundation.html">
  <link rel="alternate" type="application/rss+xml" title="TAKOBEAR" href="http://www.takobear.tw/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">TAKOBEAR</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
          <a class="page-link" href="/categories/">
            <i class="fa fa-tags"></i> 分類
          </a>

          <a class="page-link" href="/feed.xml">
            <i class="fa fa-rss-square"></i> RSS
          </a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">iPhone也能長曝光! AVFoundation與濾鏡的魔術</h1>
    <p class="post-meta"><time datetime="2014-01-12T20:42:28+08:00" itemprop="datePublished">Jan 12, 2014</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/6166088_orig.jpg" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>這照片作為封面實在手震得很厲害....不過Bear希望能傳達一個重點! 就是如何利用iOS 的framework來製作出長曝光的效果!
<p>利用Bear 很早之前研究的一個framework - AVFoundation 以及CoreImage兩項核心就能完成了喔!</p>
<p>下一篇Bear還會介紹如何使用音量鍵作為快門鍵的教學示範</p>
<p></p>
<p><div><!--BLOG_SUMMARY_END--></div></p>
<p><div>市面上已經有太多的相機App
<p>很多剛入門iOS的朋友也會想寫個把iPhone當作單眼相機使用的功能</p>
<p>老牌的類似單眼App如 Slow Shutter就有支援這樣的功能</p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/6974678_orig.png" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>它的操作界面介紹如下面兩張:</div></p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/4638105_orig.jpeg" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/7242445_orig.jpeg" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>很多剛接觸iOS , 有一些開發經驗的朋友看到這樣子的App
<p>通常直覺上會這樣想的"噢~ 看起來很簡單 Duration就是曝光時間, FPS 就是Frame per second, Sensitivity 一定就是感光度"</p>
<p>大概就是對應單眼相機最重要的三元素: 快門時間, 光圈 以及iSO吧!!!</p>
<p>這很簡單吧</p>
<p>只要找到Apple 開放iPhone這些操作的SDk就能做到啦</p>
<p>完全不需要費什麼心!</p>
<p>Bear剛開始的時候也是這麼覺得的....</p>
<p>直到後來才發現....</p>
<p>錯！！！！</p>
<p>如果這麼簡單就能做到的話....</p>
<p>也不會有人能賣1.99 USD 了(掩面哭泣)</p>
<p>事實上iPhone對於開放裝置的支援程度相當低落</p>
<p>不亂你想要調整光圈(F值在iPhone 4s 固定為2.4)</p>
<p>iso (沒有選項)</p>
<p>調整曝光時間(你想都別想)</p>
<p>基本上都是做不到的!!!</p>
<p>Apple的政策中基本上就是不允許你對於相機做一切類似單眼的調整</p>
<p>．．．</p>
<p>那這樣的App 又是怎麼做出來的呢!!?</p>
<p>等等</p>
<p>這邊我們在冷靜看一下那三個參數</p>
<p><strong>Duration</strong></p>
<p><strong>FPS</strong></p>
<p><strong>Sensitivity </strong></p>
<p>仔細看看 不曉得版友有沒有發現奇怪的地方</p>
<p>Duration作為曝光時間解釋沒有問題</p>
<p>FPS 代表每秒拍攝幾張照片, 可是我們只是要拍照而已為什麼需要調整FPS的參數!?</p>
<p>Sensitivity....雖然可以解釋為曝光度 但是為什麼會用一個這麼不接近單眼的參數數值呢?</p>
<p>理由很簡單</p>
<p>就是他們真的就有跟字面上一樣的意思....</p>
<p>他們不是單眼的參數</p>
<p>而是一個可以讓你模擬出更加接近真實長曝光的參數</p>
<p>後來可以觀察其他同類型App都會使用類似的3個參數</p>
<p>這點我們就必須認知道一件事實</p>
<p><strong>“符合這三個名字的參數調整, 能使iPhone 模擬出長曝光的效果”</strong></p>
<p>基於這些元素, Bear想到了一個可能的邏輯架構如下圖:</p>
<p></p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/5659735_orig.png" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>簡單的說我們的概念在於1. 用一個計時器錄影 (沒錯!  就是錄影, 這裡也就是duration跟fps的重點, 因為我們實際上是在錄影)
<p>2. 把一個錄影拍到的frame 儲存在一個陣列</p>
<p>3. 時間到結束儲存 (完成配置)</p>
<p>4. 把陣列裡的每一張照片疊加在一起</p>
<p>就是這麼簡單</p>
<p>只要把每一張照片疊加起來就好!</p>
<p>不過現實是光是疊加還不夠</p>
<p>我們還需要對每一張照片做特殊的處理</p>
<p>這裡必須導入Sensitivity的概念, 我們也就必須透過一些逆向工程的方法來推測出Sensitivity實際上是什麼數值</p>
<p>不過依據使用Photoshop的概念, 通常我們需要的是調整模糊程度(blur)以及透明度(opacity)</p>
<p>有了初步的規劃</p>
<p>接下來就能讓我們正式進入這段程式的架構了!</p>
<p>Bear 先表現一個預期看到的UI界面 (很醜不過基本功能都該有了^_&lt;)</p>
<p></p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/5355469_orig.png" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/5452918_orig.png" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>一個看起來很像但是不太一樣的功能界面!概念就是這樣:
<p>右邊的藍色Option按鈕按下去後, 可以跳出三個參數的選項</p>
<p>按下Done以後會記錄參數</p>
<p>最後按下Snap 會儲存在本地端的相簿內!</p>
<p>這邊我們設計出一個Singleton</p>
<p>(單件物件：又稱作Instance, 是一個在一整個App周期中只會存在唯一一個物件的存在, 以iOS內來說,</p>
<p>最常見到的就是UIApplication這個class, 他就代表這個App, 之後我們會講到一些關於他的操作)</p>
<p>接下來我們談怎麼設計囉! ( 很重要一點, Bear為求懶惰, 這邊是用ARC來建立的喔!)</p>
<p>建立一個名叫做SHCamera的物件, 繼承自NSObject</p>
<p>他的界面如下:</p>
<p>#import &lt;Foundation/Foundation.h&gt;</p>
<p> typedef void (^CameraResult) (UIImage *image);</p>
<p> typedef void (^CameraBegin) (void);</p>
<p> @interface SHCamera : NSObject</p>
<p> @property (nonatomic)           CGFloat sensitivity;</p>
<p>@property (nonatomic)           CGFloat ev;</p>
<p> @property (nonatomic)           float   exposureTime;</p>
<p> @property (nonatomic)           BOOL    isCameraBack;</p>
<p> @property (nonatomic, readonly) UIView *imgView;</p>
<p>+ (id)sharedInstance;</p>
<p>- (void)showViewInCamera;</p>
<p>- (void)stopViewInCamera;</p>
<p>- (void)setUpPreviewView:(UIView *)imgView;</p>
<p>- (void)takeSnap:(CameraBegin)begin withCompletetion:(CameraResult)result;</p>
<p> @end</p>
<p>我們這邊思考的界面概念很簡單:</p>
<p>1. sharedinstance 用來建立物件實體</p>
<p>2. showViewInCamera 與 stopViewInCamera 分別用作顯示畫面在手機App上以及停止顯示畫面在手機上的行為</p>
<p>3. setUpPreviewView:(UIView *)imgView; 這隻比較特別, Bear暫時沒想到怎麼處理它最好, 基本上他就是讓畫面的View設定為顯示攝影機畫面的method, 如果不執行的話就看不到畫面了!</p>
<p>4. - (void)takeSnap:(CameraBegin)begin withCompletetion:(CameraResult)result; 這隻method 最重要了, 只要呼叫他, 我們可以在begin這個block內撰寫 "開始拍照的動作" 跟 在result內寫"拍完照片後要做的事"</p>
<p>透過這樣的形式就能最簡單的實現“長曝光”了!!</p>
<p>從圖面來看的話, 這個物件就長得像這樣</p>
<p></p>
<p><div>
<p><div><a> <img src="http://www.takobear.weebly.com/uploads/1/9/9/7/19975491/4232314_orig.png" alt="图片" /></a>
<p><div></div></p>
<p></p>
<p></p>
<p><div>好吧小編繪畫能力實在很崩潰 （大哭）不過重點就是像上面這樣! 我們主要是對外實作出上面四個method, 跟提供該有的property可以操作就好
<p>接下來就一步一步跟著Bear往下走吧!</p>
<p>進入SHCamera.m中, 這邊很重要,</p>
<p>在project內記得要到 Build Phases - &gt;Link Binary With Libraries 加入這四個framework</p>
<p>AVFoundation, CoreImage, Accelerate 跟MediaPlayer !</p>
<p>#import "SHCamera.h"</p>
<p>#import &lt;AVFoundation/AVFoundation.h&gt;</p>
<p>#import &lt;CoreImage/CoreImage.h&gt;</p>
<p>#import &lt;Accelerate/Accelerate.h&gt;</p>
<p>#import &lt;MediaPlayer/MediaPlayer.h&gt;</p>
<p> @interface SHCamera() &lt;AVCaptureVideoDataOutputSampleBufferDelegate&gt;</p>
<p>{</p>
<p>     // Device Session</p>
<p>AVCaptureSession *session;</p>
<p>AVCaptureDevice  *cameraDevice;</p>
<p>CIContext        *imageContext;</p>
<p>CIVector         *alphaVector;</p>
<p>CGFloat          alpha;</p>
<p>     // Mixing Filter</p>
<p>CIFilter *blendFilter;</p>
<p>CIFilter *mixFilter;</p>
<p>CIFilter *alphaFilterInput;</p>
<p>CIFilter *outCompositing;</p>
<p>CIFilter *contrastFilter;</p>
<p>CIFilter *gammaFilter;</p>
<p>CIFilter *fStopFilter;</p>
<p>     // Image Session</p>
<p>NSMutableArray  *imageArray;</p>
<p>CIImage         *tmpSnapImage;</p>
<p>// Control Setting</p>
<p>BOOL startToTakePhotos;</p>
<p>int  imageCounterCount;</p>
<p>     // Result Block to pass result</p>
<p>CameraResult resultBlock;</p>
<p>}</p>
<p>- (id)initWithCamera;</p>
<p>- (void)setUpFilter;</p>
<p>- (void)setUpCamera;</p>
<p>- (AVCaptureVideoPreviewLayer *) previewInView: (UIView *) view;</p>
<p>- (void)timerSnap:(NSTimer *)timer;</p>
<p> @end</p>
<p>我們有五個private method, 這是不希望給外面看到的XD</p>
<p>接下來一個一個介紹吧!!</p>
<p>@implementation SHCamera</p>
<p>@synthesize sensitivity  = _sensitivity;</p>
<p> @synthesize ev           = _ev;</p>
<p> @synthesize exposureTime = _exposureTime;</p>
<p> @synthesize isCameraBack = _isCameraBack;</p>
<p> @synthesize imgView      = _imgView;</p>
<p>//  這邊是標準要產生Singleton的寫法! dispatch_once 能確保這一段的程式碼在整個App中只會被執行到一次// 也就是說這個物件只會被init一次! 所有的property 或是傳輸就都能透過他進行了</p>
<p>+ (id)sharedInstance</p>
<p>{</p>
<p>static SHCamera *camrea = nil;</p>
<p>static dispatch_once_t onceToken;</p>
<p>dispatch_once(&amp;onceToken, ^{</p>
<p>camrea = [[SHCamera alloc] initWithCamera];</p>
<p>});</p>
<p>return camrea;</p>
<p>}</p>
<p>//  這邊很簡單, 就是單純的起始化</p>
<p>- (id)initWithCamera</p>
<p>{</p>
<p>self = [super init];</p>
<p>if (self) {</p>
<p>         // Basic parameters</p>
<p>imageContext = [CIContext contextWithOptions:nil];</p>
<p>imageArray   = [NSMutableArray array];</p>
<p>         // Parameter to record</p>
<p>startToTakePhotos = NO;</p>
<p>         // Default parameters</p>
<p>_sensitivity    = 1.0f;</p>
<p>_exposureTime   = 0.5f;</p>
<p>_ev             = 2.0f;</p>
<p>_isCameraBack   = YES;  // 我們預設都是用背後的Camera</p>
<p>[self setUpFilter];</p>
<p>[self setUpCamera];</p>
<p>}</p>
<p>return self;</p>
<p>}</p>
<p>接下來就是一連串的設定Fileter跟利用AVFoundation 設定攝影機的步驟</p>
<p>- (void)setUpFilter</p>
<p>{</p>
<p>blendFilter         = [CIFilter filterWithName:@"CILightenBlendMode"];</p>
<p>mixFilter           = [CIFilter filterWithName:@"CIMaximumCompositing"];</p>
<p>alphaFilterInput    = [CIFilter filterWithName:@"CIColorMatrix"];</p>
<p>contrastFilter      = [CIFilter filterWithName:@"CIColorControls"];</p>
<p>[contrastFilter setDefaults];</p>
<p>fStopFilter         = [CIFilter filterWithName:@"CIExposureAdjust"];</p>
<p>[fStopFilter setDefaults];</p>
<p>gammaFilter         = [CIFilter filterWithName:@"CIGammaAdjust"];</p>
<p>[gammaFilter setValue:[NSNumber numberWithFloat:1.0f] forKey:@"inputPower"];</p>
<p>alphaVector         = [CIVector vectorWithX:0 Y:0 Z:0 W:1.0f];</p>
<p>}</p>
<p>- (void)setUpCamera</p>
<p>{</p>
<p>     // In ARC. we do not have to handle its release problem</p>
<p>session = [[AVCaptureSession alloc] init];</p>
<p>     // Set resolution</p>
<p>     session.sessionPreset = AVCaptureSessionPresetMedium;</p>
<p>[session beginConfiguration]; // Begin to setup session</p>
<p>NSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];</p>
<p>if (_isCameraBack) {</p>
<p>         // Set up back camera</p>
<p>for (AVCaptureDevice *device in devices) {</p>
<p>if (device.position == AVCaptureDevicePositionBack) {</p>
<p>cameraDevice = device;</p>
<p>break;</p>
<p>}</p>
<p>}</p>
<p>} else {</p>
<p>for (AVCaptureDevice *device in devices) {</p>
<p>      // Setup front camera</p>
<p>      if (device.position == AVCaptureDevicePositionFront) {</p>
<p>cameraDevice = device;</p>
<p>break;</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>     // Add Input device</p>
<p>NSError *error;</p>
<p>AVCaptureDeviceInput *input = [AVCaptureDeviceInput deviceInputWithDevice:cameraDevice error:&amp;error];</p>
<p>[session addInput:input];</p>
<p>     // Add output device</p>
<p>AVCaptureVideoDataOutput *output = [[AVCaptureVideoDataOutput alloc] init];</p>
<p>[output setAlwaysDiscardsLateVideoFrames:YES];</p>
<p>[output setSampleBufferDelegate:self queue:dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_BACKGROUND, 0)];</p>
<p>output.videoSettings = [NSDictionary dictionaryWithObject:[NSNumber numberWithInt:kCVPixelFormatType_32BGRA] forKey:(id)kCVPixelBufferPixelFormatTypeKey];</p>
<p>[session addOutput:output];</p>
<p>[session commitConfiguration];</p>
<p>}</p>
<p>呼....到這邊基本的設定完成了, 我們就要繼續往下把其他method給實作出來囉!</p>
<p>以下兩隻method很簡單, 只要單純讓session 跑或不跑而已</p>
<p>- (void)showViewInCamera</p>
<p>{</p>
<p>  // Start to show image</p>
<p>  [session startRunning];</p>
<p>}</p>
<p>- (void)stopViewInCamera</p>
<p>{</p>
<p>     // Stop configuring image</p>
<p>[session stopRunning];</p>
<p>}</p>
<p>接下來是比較大的重點, 我們要指定畫面的某一個View 能顯現攝影機的畫面, 必須透過以下三個method的串聯才行!</p>
<p>- (void)setUpPreviewView:(UIView *)imgView</p>
<p>{</p>
<p>if (!session) {</p>
<p>NSLog(@"There is no session");</p>
<p>return;</p>
<p>}</p>
<p>_imgView = imgView;</p>
<p>AVCaptureVideoPreviewLayer *previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:session];</p>
<p>previewLayer.frame = CGRectMake(0, 0, _imgView.frame.size.width, _imgView.frame.size.height);</p>
<p>previewLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;</p>
<p>     // Show preview layer on this image view</p>
<p>     [_imgView.layer addSublayer:previewLayer];</p>
<p>     // Set the orientation layout subview</p>
<p>     [self layoutPreviewInView];</p>
<p>}</p>
<p>- (AVCaptureVideoPreviewLayer *) previewInView: (UIView *) view</p>
<p>{</p>
<p>for (CALayer *layer in view.layer.sublayers)</p>
<p>if ([layer isKindOfClass:[AVCaptureVideoPreviewLayer class]])</p>
<p>return (AVCaptureVideoPreviewLayer *)layer;</p>
<p>return nil;</p>
<p>}</p>
<p>- (void) layoutPreviewInView</p>
<p>{</p>
<p>AVCaptureVideoPreviewLayer *layer = [self previewInView:_imgView];</p>
<p>if (!layer) return;</p>
<p>UIDeviceOrientation orientation = [UIDevice currentDevice].orientation;</p>
<p>CATransform3D transform = CATransform3DIdentity;</p>
<p>if (orientation == UIDeviceOrientationPortrait) ;</p>
<p>else if (orientation == UIDeviceOrientationLandscapeLeft)</p>
<p>transform = CATransform3DMakeRotation(-M_PI_2, 0.0f, 0.0f, 1.0f);</p>
<p>else if (orientation == UIDeviceOrientationLandscapeRight)</p>
<p>transform = CATransform3DMakeRotation(M_PI_2, 0.0f, 0.0f, 1.0f);</p>
<p>else if (orientation == UIDeviceOrientationPortraitUpsideDown)</p>
<p>transform = CATransform3DMakeRotation(M_PI, 0.0f, 0.0f, 1.0f);</p>
<p>layer.transform = transform;</p>
<p>layer.frame = CGRectMake(0, 0, _imgView.frame.size.width, _imgView.frame.size.height);</p>
<p>}</p>
<p>接下來我們就要進入重點中的重點啦!!!!(拖了很久總算要進入正題了....)</p>
<p>#pragma mark - AVCaptureVideoDataOutputSampleBufferDelegate</p>
<p>- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection</p>
<p>{</p>
<p>     @autoreleasepool {</p>
<p>CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);</p>
<p>CFDictionaryRef attachments = CMCopyDictionaryOfAttachments(kCFAllocatorDefault, sampleBuffer, kCMAttachmentMode_ShouldPropagate);</p>
<p>CIImage *image = [[CIImage alloc] initWithCVPixelBuffer:pixelBuffer options:(__bridge_transfer NSDictionary *)attachments];</p>
<p>image = [image imageByApplyingTransform:CGAffineTransformMakeRotation(-M_PI_2)];</p>
<p>CGPoint origin = [image extent].origin;</p>
<p>image = [image imageByApplyingTransform:CGAffineTransformMakeTranslation(-origin.x, -origin.y)];</p>
<p>         // Start to record image</p>
<p>if (startToTakePhotos == YES) {</p>
<p>           // Add image to image array</p>
<p>[imageArray addObject:image];</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p>這邊做法是這樣的, 我們會不斷去讀取frame buffer (利用這個的AVCaptureVideoDataOutputSampleBufferDelegate method, iOS SDK會自動幫你把一張攝影機拍到的frame取出來)</p>
<p>然後不斷去檢查startToTakePhotos這個布林變數</p>
<p>只要他是YES的狀態</p>
<p>我們就把目前的frame 一直儲存到imageArray內</p>
<p>這樣就能完成以下的步驟:</p>
<p>觸發開始拍照後 - &gt;計時 -&gt; 儲存相片 - &gt; 計時時間到 - &gt; 合成相片</p>
<p>一張長曝光的合成照片就能完成了喔!</p>
<p>接下來就是最後的步驟了!!</p>
<p>#pragma mark - Action Method</p>
<p>- (void)takeSnap:(CameraBegin)begin withCompletetion:(CameraResult)result</p>
<p>{</p>
<p>resultBlock = [result copy];</p>
<p>startToTakePhotos = YES;</p>
<p>     // Execute the beginning block</p>
<p>begin();</p>
<p>     // Set up timer to record</p>
<p>[NSTimer scheduledTimerWithTimeInterval:_exposureTime target:self selector:@selector(timerSnap:) userInfo:nil repeats:NO];</p>
<p>}</p>
<p>-(void)timerSnap:(NSTimer *)timer</p>
<p>{</p>
<p>startToTakePhotos = NO;</p>
<p>[timer invalidate];</p>
<p>timer = nil;</p>
<p>     // The first image</p>
<p>CIImage *firstSnapImage = [imageArray objectAtIndex:0];</p>
<p>     // BlockImage</p>
<p>CIImage *backImage = firstSnapImage;</p>
<p>CIImage *inputImage = [[CIImage alloc] init];</p>
<p>CGImageRef tmpRef   = [imageContext createCGImage:backImage fromRect:backImage.extent];</p>
<p>UIImage *imageBack  = [[UIImage alloc] initWithCGImage:tmpRef];</p>
<p>UIImage *imageInput = [[UIImage alloc] init];</p>
<p>for (int i = 0; i &lt; imageArray.count; i++) {</p>
<p>         // Add Autorealse pool to release imageref, without release, your app will definitely crash</p>
<p> @autoreleasepool {</p>
<p>             // Get the CIImage in each of item for imageArray</p>
<p>inputImage = [imageArray objectAtIndex:i];</p>
<p>             // Transfer CIImage to CIImageRef</p>
<p>CGImageRef inputImageRef = [imageContext createCGImage:inputImage fromRect:inputImage.extent];</p>
<p>             // Transfer CIImage to UIImage</p>
<p>imageInput = [UIImage imageWithCGImage:inputImageRef];</p>
<p>             // Set up the two CIImage</p>
<p>backImage   = [[CIImage alloc] initWithImage:imageBack];</p>
<p>inputImage  = [[CIImage alloc] initWithImage:imageInput];</p>
<p>             // Actually, the value for sensitivity is just the alpha or opacity value</p>
<p>// Set up the CIImage filter chain</p>
<p>// 1. Alpha opacity filter</p>
<p>alpha = _sensitivity;</p>
<p>alphaVector = [CIVector vectorWithX:0 Y:0 Z:0 W:alpha];</p>
<p>[alphaFilterInput setValue:alphaVector forKey:@"inputAVector"];</p>
<p>[alphaFilterInput setValue:inputImage forKey:@"inputImage"];</p>
<p>inputImage = alphaFilterInput.outputImage;</p>
<p>             // 2. Enhance Contrast</p>
<p>[contrastFilter setValue:inputImage forKey:@"inputImage"];</p>
<p>[contrastFilter setValue:[NSNumber numberWithFloat:1.1f] forKey:@"inputContrast"];</p>
<p>[contrastFilter setValue:[NSNumber numberWithFloat:0.05f] forKey:@"inputBrightness"];</p>
<p>inputImage = contrastFilter.outputImage;</p>
<p>             // 3. Add Gamma</p>
<p>[gammaFilter setValue:inputImage forKey:@"inputImage"];</p>
<p>inputImage = gammaFilter.outputImage;</p>
<p>             // 4. Blend backimage and inputimage</p>
<p>[blendFilter setValue:backImage forKey:@"inputBackgroundImage"];</p>
<p>[blendFilter setValue:inputImage forKey:@"inputImage"];</p>
<p>backImage = blendFilter.outputImage;</p>
<p>             // 5. Release all image</p>
<p>CGImageRef backRef = [imageContext createCGImage:backImage fromRect:backImage.extent];</p>
<p>imageBack =  [UIImage imageWithCGImage:backRef];</p>
<p>CGImageRelease(inputImageRef);</p>
<p>CGImageRelease(backRef);</p>
<p>imageInput = nil;</p>
<p>inputImage = nil;</p>
<p>}</p>
<p>}</p>
<p>     // Bloom Effect</p>
<p>[fStopFilter setValue:[NSNumber numberWithFloat:_ev] forKey:@"inputEV"];</p>
<p>[fStopFilter setValue:backImage forKey:@"inputImage"];</p>
<p>backImage = fStopFilter.outputImage;</p>
<p>     // Compositing the firstImage with the blurred image</p>
<p>[mixFilter setValue:firstSnapImage forKey:@"inputBackgroundImage"];</p>
<p>[mixFilter setValue:backImage forKey:@"inputImage"];</p>
<p>backImage = mixFilter.outputImage;</p>
<p>     // Finally, Generate the UIImage</p>
<p>CGImageRef cgImageOutPut = [imageContext createCGImage:backImage fromRect:backImage.extent];</p>
<p>UIImage *resultImage = [UIImage imageWithCGImage:cgImageOutPut];</p>
<p>    // Pass the result image to the block, and user can use this result to do things they want to do</p>
<p>resultBlock(resultImage);</p>
<p>     // Reset the image array</p>
<p>imageArray = [NSMutableArray array];</p>
<p>     // Release the rest image resource</p>
<p>imageBack = nil;</p>
<p>backImage = nil;</p>
<p>firstSnapImage = nil;</p>
<p>resultImage = nil;</p>
<p>CGImageRelease(cgImageOutPut);</p>
<p>CGImageRelease(tmpRef);</p>
<p>}</p>
<p>@end</p>
<p>詳細濾鏡的步驟大家看上面的Code就行囉!!</p>
<p>我們利用block變數來處理拍攝前與拍攝後的動作</p>
<p>這樣可以在畫面上放上一些比如警告標示之類的!</p>
<p>以下可以看到Bear拍攝的一些成果參考看看!</p>
<p></p>
<p><div>
<p><div></div></p>
<p><div id="955737914299147592-slideshow"></div></p>
<p><script type="text/javascript">// < ![CDATA[</p>
<p> (function(jQuery) { function init() { wSlideshow.render({elementID:"955737914299147592",nav:"thumbnails",navLocation:"bottom",captionLocation:"bottom",transition:"fade",autoplay:"0",speed:"5",aspectRatio:"auto",showControls:"true",randomStart:"false",images:[{"url":"1\/9\/9\/7\/19975491\/1912652.jpg","width":187,"height":250,"fullHeight":480,"fullWidth":360},{"url":"1\/9\/9\/7\/19975491\/9861483.jpg","width":187,"height":250,"fullHeight":480,"fullWidth":360},{"url":"1\/9\/9\/7\/19975491\/4342192.jpg","width":187,"height":250,"fullHeight":480,"fullWidth":360},{"url":"1\/9\/9\/7\/19975491\/970851.jpg","width":187,"height":250,"fullHeight":480,"fullWidth":360},{"url":"1\/9\/9\/7\/19975491\/681323.jpg","width":187,"height":250,"fullHeight":480,"fullWidth":360},{"url":"1\/9\/9\/7\/19975491\/8361983.jpg","width":187,"height":250,"fullHeight":643,"fullWidth":483}]}) } jQuery ? jQuery(init) : document.observe('dom:loaded', init) })(window._W &#038;& _W.jQuery)</p>
<p>// ]]></script></p>
<p><div></div></p>
<p></p>
<p><div>Bear 拍攝的時候並沒有使用腳架!!
<p>手震是非常明顯的QQ如果有興趣的版友可以到github上參考完整的code喔!</p>
<p><a href="https://github.com/shouian/SHLongExposureDemo" target="_blank">https://github.com/shouian/SHLongExposureDemo</a></p>
<p>有任何疑問歡迎來信到我們的信箱</p>
<p>有興趣的朋友請繼續關注TakoBear的新消息!!!</p>
<p>近期我們也會整理更多開發文章與有趣的新聞分享給大家!</p>
<p></p>
</div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p></div></p>

  </div>

</article>


<div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
        this.page.url = "http:\/\/www.takobear.tw\/2014\/01\/12\/iphone-avfoundation\/";
        this.page.identifier = "729 http:\/\/www.takobear.tw\/201702608526356260322804024687\/iphone-avfoundation";
    };

    (function() {
        var d = document, s = d.createElement('script');

        s.src = '//takobear.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-2">
        <ul class="contact-list">
          <li>TAKOBEAR</li>
          <li><a href="mailto:takobearx@gmail.com">takobearx@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-1 footer-col-right">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/TakoBear">
  <span class="icon"><i class="fa fa-github fa-fw"></i></span>
  <span class="username">TakoBear</span>
</a>

          </li>
          

          
          <li>
            <a href="https://www.facebook.com/funnytechandnews">
  <span class="icon"><i class="fa fa-facebook fa-fw"></i></span>
  <span class="username">funnytechandnews</span>
</a>

          </li>
          
        </ul>
      </div>
    </div>

    <div class="footer-license">
      2013-2016 TakoBear. All rights reserved.
    </div>
  </div>

</footer>


  </body>

</html>
